{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = 'SAME'\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 1\n",
    "conv2_pad = 'SAME'\n",
    "\n",
    "conv3_fmaps = 128\n",
    "conv3_ksize = 3\n",
    "conv3_stride = 1\n",
    "conv3_pad = 'SAME'\n",
    "\n",
    "conv4_fmaps = 256\n",
    "conv4_ksize = 3\n",
    "conv4_stride = 1\n",
    "conv4_pad = 'SAME'\n",
    "\n",
    "pool1_fmaps = conv1_fmaps\n",
    "pool2_fmaps = conv2_fmaps\n",
    "pool3_fmaps = conv3_fmaps\n",
    "pool4_fmaps = conv4_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 1, 256)\n",
      "(?, 256)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('inputs'):\n",
    "    X = tf.placeholder(name='X', dtype=tf.float32, shape=(None, n_inputs))\n",
    "    X_reshaped = tf.reshape(X, shape=(-1, height, width, channels))\n",
    "    y = tf.placeholder(name='y', dtype=tf.int32, shape=[None])\n",
    "    \n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, strides=conv1_stride,\n",
    "                         padding=conv1_pad, activation=tf.nn.relu, name='conv1')\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides = [1,2,2,1], padding='VALID')\n",
    "\n",
    "conv2 = tf.layers.conv2d(pool1, filters=conv2_fmaps, kernel_size=conv2_ksize, strides=conv2_stride, \n",
    "                        padding=conv2_pad, activation=tf.nn.relu, name='conv2')\n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    "conv3 = tf.layers.conv2d(pool2, filters=conv3_fmaps, kernel_size=conv3_ksize, strides=conv3_stride, \n",
    "                        padding=conv3_pad, activation=tf.nn.relu, name='conv3')\n",
    "\n",
    "pool3 = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "conv4 = tf.layers.conv2d(pool3, filters=conv4_fmaps, kernel_size=conv4_ksize, strides=conv4_stride,\n",
    "                        padding=conv4_pad, activation=tf.nn.relu, name='conv4')\n",
    "with tf.name_scope('pool4'):\n",
    "    pool4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    pool4_flat = tf.reshape(pool4, shape=[-1, pool4_fmaps * 1 * 1])\n",
    "print(pool4.shape)\n",
    "print(pool4_flat.shape)\n",
    "\n",
    "with tf.name_scope('fc1'):\n",
    "    fc1 = tf.layers.dense(pool4_flat, n_fc1, activation=tf.nn.relu, name='fc1')\n",
    "    \n",
    "with tf.name_scope('output'):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name='output')\n",
    "    y_prob = tf.nn.softmax(logits, name='y_prob')\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "with tf.name_scope('init_and_save'):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9749bf5e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "x = mnist.train.images[10]\n",
    "x = x.reshape((28, 28))\n",
    "plt.imshow(x, cmap=cm.binary)\n",
    "plt.show()\n",
    "#plt.imshow(mnist.train.images[1].reshape((height, width, channels)))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training accuracy: 0.9900000095367432, validation accuracy: 0.9846000075340271\n",
      "epoch: 1, training accuracy: 0.9800000190734863, validation accuracy: 0.9846000075340271\n",
      "epoch: 2, training accuracy: 1.0, validation accuracy: 0.9887999892234802\n",
      "epoch: 3, training accuracy: 0.9900000095367432, validation accuracy: 0.9854000210762024\n",
      "epoch: 4, training accuracy: 1.0, validation accuracy: 0.993399977684021\n",
      "epoch: 5, training accuracy: 1.0, validation accuracy: 0.9905999898910522\n",
      "epoch: 6, training accuracy: 1.0, validation accuracy: 0.9905999898910522\n",
      "epoch: 7, training accuracy: 1.0, validation accuracy: 0.991599977016449\n",
      "epoch: 8, training accuracy: 1.0, validation accuracy: 0.9922000169754028\n",
      "epoch: 9, training accuracy: 1.0, validation accuracy: 0.9937999844551086\n",
      "epoch: 10, training accuracy: 1.0, validation accuracy: 0.9926000237464905\n",
      "epoch: 11, training accuracy: 1.0, validation accuracy: 0.9929999709129333\n",
      "epoch: 12, training accuracy: 1.0, validation accuracy: 0.9926000237464905\n",
      "epoch: 13, training accuracy: 1.0, validation accuracy: 0.9886000156402588\n",
      "epoch: 14, training accuracy: 1.0, validation accuracy: 0.9918000102043152\n",
      "epoch: 15, training accuracy: 1.0, validation accuracy: 0.9927999973297119\n",
      "epoch: 16, training accuracy: 1.0, validation accuracy: 0.9929999709129333\n",
      "epoch: 17, training accuracy: 1.0, validation accuracy: 0.9932000041007996\n",
      "epoch: 18, training accuracy: 1.0, validation accuracy: 0.9922000169754028\n",
      "epoch: 19, training accuracy: 1.0, validation accuracy: 0.9936000108718872\n",
      "epoch: 20, training accuracy: 1.0, validation accuracy: 0.9929999709129333\n",
      "epoch: 21, training accuracy: 1.0, validation accuracy: 0.9922000169754028\n",
      "epoch: 22, training accuracy: 1.0, validation accuracy: 0.9927999973297119\n",
      "epoch: 23, training accuracy: 1.0, validation accuracy: 0.9929999709129333\n",
      "epoch: 26, training accuracy: 1.0, validation accuracy: 0.989799976348877\n",
      "epoch: 27, training accuracy: 1.0, validation accuracy: 0.9894000291824341\n",
      "epoch: 28, training accuracy: 1.0, validation accuracy: 0.9929999709129333\n",
      "epoch: 29, training accuracy: 1.0, validation accuracy: 0.9908000230789185\n",
      "epoch: 30, training accuracy: 1.0, validation accuracy: 0.9944000244140625\n",
      "epoch: 31, training accuracy: 1.0, validation accuracy: 0.9936000108718872\n",
      "epoch: 32, training accuracy: 1.0, validation accuracy: 0.9940000176429749\n",
      "epoch: 33, training accuracy: 1.0, validation accuracy: 0.9929999709129333\n",
      "epoch: 34, training accuracy: 1.0, validation accuracy: 0.9914000034332275\n",
      "epoch: 35, training accuracy: 1.0, validation accuracy: 0.9937999844551086\n",
      "epoch: 36, training accuracy: 1.0, validation accuracy: 0.9923999905586243\n",
      "epoch: 37, training accuracy: 1.0, validation accuracy: 0.9911999702453613\n",
      "epoch: 38, training accuracy: 1.0, validation accuracy: 0.9926000237464905\n",
      "epoch: 39, training accuracy: 1.0, validation accuracy: 0.9901999831199646\n",
      "epoch: 40, training accuracy: 1.0, validation accuracy: 0.9926000237464905\n",
      "epoch: 41, training accuracy: 1.0, validation accuracy: 0.9932000041007996\n",
      "epoch: 42, training accuracy: 1.0, validation accuracy: 0.9932000041007996\n",
      "epoch: 43, training accuracy: 1.0, validation accuracy: 0.9922000169754028\n",
      "epoch: 44, training accuracy: 1.0, validation accuracy: 0.9940000176429749\n",
      "epoch: 45, training accuracy: 0.9900000095367432, validation accuracy: 0.991599977016449\n",
      "epoch: 46, training accuracy: 1.0, validation accuracy: 0.9936000108718872\n",
      "epoch: 47, training accuracy: 1.0, validation accuracy: 0.9940000176429749\n",
      "epoch: 48, training accuracy: 1.0, validation accuracy: 0.9947999715805054\n",
      "epoch: 49, training accuracy: 1.0, validation accuracy: 0.9947999715805054\n",
      "epoch: 50, training accuracy: 1.0, validation accuracy: 0.9947999715805054\n",
      "epoch: 51, training accuracy: 1.0, validation accuracy: 0.9947999715805054\n",
      "epoch: 52, training accuracy: 1.0, validation accuracy: 0.9947999715805054\n",
      "epoch: 53, training accuracy: 1.0, validation accuracy: 0.9947999715805054\n",
      "epoch: 54, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 55, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 56, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 57, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 58, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 59, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 60, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 61, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 62, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 63, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 64, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 65, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 66, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 67, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 68, training accuracy: 1.0, validation accuracy: 0.9945999979972839\n",
      "epoch: 69, training accuracy: 1.0, validation accuracy: 0.9944000244140625\n",
      "epoch: 70, training accuracy: 1.0, validation accuracy: 0.9944000244140625\n",
      "epoch: 71, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 72, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 73, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 74, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 75, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 76, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 77, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 78, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 79, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 80, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 81, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 82, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 83, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 84, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 85, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 86, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 87, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 88, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 89, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 90, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 91, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 92, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 93, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 94, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 95, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 96, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 97, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 98, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n",
      "epoch: 99, training accuracy: 1.0, validation accuracy: 0.9941999912261963\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "prev_val = 0\n",
    "stopping_criterion = 0\n",
    "thr = 15\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size=batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "        print('epoch: {}, training accuracy: {}, validation accuracy: {}'.format(epoch, acc_train, acc_val))\n",
    "        if acc_val < prev_val * (1 + thr / 100):\n",
    "            stopping_criterion += 1\n",
    "            if stopping_criterion >= 3:\n",
    "                print('Early stopping invoked!')\n",
    "                break\n",
    "        #prev_val = acc_val\n",
    "        saver_path = saver.save(sess, '/tmp/mnist_with_cnn3.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/mnist_with_cnn3.ckpt\n",
      "0.9947\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '/tmp/mnist_with_cnn3.ckpt')\n",
    "    Z = logits.eval(feed_dict= {X: mnist.test.images})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "    print(np.mean(y_pred == mnist.test.labels).astype(np.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
